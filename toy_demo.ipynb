{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
    "    '''\n",
    "    Compute the gradient of `outputs` with respect to `inputs`\n",
    "\n",
    "    gradient(x.sum(), x)\n",
    "    gradient((x * y).sum(), [x, y])\n",
    "    '''\n",
    "    if torch.is_tensor(inputs):\n",
    "        inputs = [inputs]\n",
    "    else:\n",
    "        inputs = list(inputs)\n",
    "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
    "                                allow_unused=True,\n",
    "                                retain_graph=retain_graph,\n",
    "                                create_graph=create_graph)\n",
    "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
    "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
    "\n",
    "\n",
    "def jacobian(outputs, inputs, create_graph=False,retain_graph=True):\n",
    "    '''\n",
    "    Compute the Jacobian of `outputs` with respect to `inputs`\n",
    "\n",
    "    jacobian(x, x)\n",
    "    jacobian(x * y, [x, y])\n",
    "    jacobian([x * y, x.sqrt()], [x, y])\n",
    "    '''\n",
    "    if torch.is_tensor(outputs):\n",
    "        outputs = [outputs]\n",
    "    else:\n",
    "        outputs = list(outputs)\n",
    "\n",
    "    if torch.is_tensor(inputs):\n",
    "        inputs = [inputs]\n",
    "    else:\n",
    "        inputs = list(inputs)\n",
    "\n",
    "    jac = []\n",
    "    for output in outputs:\n",
    "        output_flat = output.view(-1)\n",
    "        output_grad = torch.zeros_like(output_flat)\n",
    "        for i in range(len(output_flat)):\n",
    "            output_grad[i] = 1\n",
    "            jac += [gradient(output_flat, inputs, output_grad, retain_graph, create_graph)]\n",
    "            output_grad[i] = 0\n",
    "    return torch.stack(jac)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 10])"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "inputs = torch.randn(10, requires_grad=True)\n",
    "net = nn.Linear(10, 3)\n",
    "outputs = net(inputs)\n",
    "outputs_flat = outputs.view(-1)\n",
    "outputs_grad = torch.zeros_like(outputs_flat)\n",
    "jac = []\n",
    "for i in range(len(outputs_flat)):\n",
    "    outputs_grad[i] = 1\n",
    "    jac += [gradient(outputs_flat, inputs, outputs_grad, retain_graph=True, create_graph=False)]\n",
    "    outputs_grad[i] = 0\n",
    "\n",
    "torch.stack(jac).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Levenberg-Marquardt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(Optimizer):\n",
    "    '''\n",
    "    Arguments:\n",
    "        lr: learning rate (step size) default:1\n",
    "        alpha: the hyperparameter in the regularization default:0.2\n",
    "    '''\n",
    "    def __init__(self, params, lr=1):\n",
    "        defaults = dict(\n",
    "            lr = lr\n",
    "            #alpha = alpha\n",
    "        )\n",
    "        super(LM, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError (\"LM doesn't support per-parameter options\")    \n",
    "    \n",
    "    def step(self, f, J, alpha, closure=None):\n",
    "        '''\n",
    "        performs a single step\n",
    "\n",
    "        '''\n",
    "        assert len(self.param_groups) == 1\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        # approximate Hessian\n",
    "        H = torch.matmul(J.T, J) + torch.eye(J.shape[-1]).to(device) * alpha\n",
    "        # calculate the update       \n",
    "        delta_w = -1 * torch.matmul(torch.inverse(H), torch.matmul(J.T, f)).detach()\n",
    "        offset = 0\n",
    "        for p in group['params']:\n",
    "            numel = p.numel()\n",
    "            p = p + lr * delta_w[offset:offset + numel].view_as(p)\n",
    "            offset += numel\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1, 50),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(50, 20),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(20, 1),\n",
    "    )\n",
    "net.cuda('cuda:0')\n",
    "\n",
    "optimizer = LM(net.parameters())\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  \n",
    "y = torch.sin(x) + 0.2*torch.rand(x.size())\n",
    "torch_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\nMSE loss\n0.07400921732187271\nsuccessful iteration\n1\nMSE loss\n0.10278677195310593\n2\nMSE loss\n0.17369677126407623\n3\nMSE loss\n0.08175666630268097\nsuccessful iteration\n4\nMSE loss\n0.11438998579978943\n5\nMSE loss\n0.05649910122156143\nsuccessful iteration\n6\nMSE loss\n0.03103026933968067\nsuccessful iteration\n7\nMSE loss\n0.02678677812218666\nsuccessful iteration\n8\nMSE loss\n0.02312987670302391\nsuccessful iteration\n9\nMSE loss\n0.01499958522617817\nsuccessful iteration\n10\nMSE loss\n0.014151274226605892\nsuccessful iteration\n11\nMSE loss\n0.016966229304671288\n12\nMSE loss\n0.028560999780893326\n13\nMSE loss\n0.021453185006976128\nsuccessful iteration\n14\nMSE loss\n0.030976083129644394\n15\nMSE loss\n0.020725639536976814\nsuccessful iteration\n16\nMSE loss\n4.375731805339456e-06\nsuccessful iteration\n17\nMSE loss\n0.00869146827608347\n18\nMSE loss\n0.013908074237406254\n19\nMSE loss\n0.0004175693611614406\nsuccessful iteration\n20\nMSE loss\n0.0021089829970151186\n21\nMSE loss\n0.00942202564328909\n22\nMSE loss\n0.004964614752680063\nsuccessful iteration\n23\nMSE loss\n0.01895262487232685\n24\nMSE loss\n0.017586054280400276\nsuccessful iteration\n25\nMSE loss\n0.02936902455985546\n26\nMSE loss\n0.0036246185190975666\nsuccessful iteration\n27\nMSE loss\n0.0005024581332691014\nsuccessful iteration\n28\nMSE loss\n0.04279839247465134\n29\nMSE loss\n0.028177369385957718\nsuccessful iteration\n30\nMSE loss\n0.048177387565374374\n31\nMSE loss\n0.057009030133485794\n32\nMSE loss\n0.08108218014240265\n33\nMSE loss\n0.057300060987472534\nsuccessful iteration\n34\nMSE loss\n0.0929064229130745\n35\nMSE loss\n0.04551120102405548\nsuccessful iteration\n36\nMSE loss\n0.04342041537165642\nsuccessful iteration\n37\nMSE loss\n0.07588854432106018\n38\nMSE loss\n0.07534178346395493\nsuccessful iteration\n39\nMSE loss\n0.19364799559116364\n40\nMSE loss\n0.1973327398300171\n41\nMSE loss\n0.126874178647995\nsuccessful iteration\n42\nMSE loss\n0.20644816756248474\n43\nMSE loss\n0.21091486513614655\n44\nMSE loss\n0.2373080849647522\n45\nMSE loss\n0.2285441905260086\nsuccessful iteration\n46\nMSE loss\n0.21975316107273102\nsuccessful iteration\n47\nMSE loss\n0.2975119650363922\n48\nMSE loss\n0.3676622807979584\n49\nMSE loss\n0.21725158393383026\nsuccessful iteration\n50\nMSE loss\n0.2113315314054489\nsuccessful iteration\n51\nMSE loss\n0.2301803082227707\n52\nMSE loss\n0.30093464255332947\n53\nMSE loss\n0.2626749873161316\nsuccessful iteration\n54\nMSE loss\n0.3929395079612732\n55\nMSE loss\n0.40407127141952515\n56\nMSE loss\n0.3561294972896576\nsuccessful iteration\n57\nMSE loss\n0.5225480198860168\n58\nMSE loss\n0.3895610272884369\nsuccessful iteration\n59\nMSE loss\n0.551679253578186\n60\nMSE loss\n0.4203908145427704\nsuccessful iteration\n61\nMSE loss\n0.44140762090682983\n62\nMSE loss\n0.6391379833221436\n63\nMSE loss\n0.46044206619262695\nsuccessful iteration\n64\nMSE loss\n0.6251867413520813\n65\nMSE loss\n0.6343669891357422\n66\nMSE loss\n0.6283606886863708\nsuccessful iteration\n67\nMSE loss\n0.6802140474319458\n68\nMSE loss\n0.7548103928565979\n69\nMSE loss\n0.7395333647727966\nsuccessful iteration\n70\nMSE loss\n0.5764968395233154\nsuccessful iteration\n71\nMSE loss\n0.9051216840744019\n72\nMSE loss\n0.8253471255302429\nsuccessful iteration\n73\nMSE loss\n0.6680349707603455\nsuccessful iteration\n74\nMSE loss\n0.8678345084190369\n75\nMSE loss\n0.6710386872291565\nsuccessful iteration\n76\nMSE loss\n1.010837197303772\n77\nMSE loss\n1.0292373895645142\n78\nMSE loss\n1.0650807619094849\n79\nMSE loss\n0.9840342998504639\nsuccessful iteration\n80\nMSE loss\n1.0269453525543213\n81\nMSE loss\n1.0967915058135986\n82\nMSE loss\n0.9563469886779785\nsuccessful iteration\n83\nMSE loss\n0.9298856854438782\nsuccessful iteration\n84\nMSE loss\n1.090193748474121\n85\nMSE loss\n0.941259503364563\nsuccessful iteration\n86\nMSE loss\n1.0372525453567505\n87\nMSE loss\n0.9798269271850586\nsuccessful iteration\n88\nMSE loss\n0.9797974228858948\nsuccessful iteration\n89\nMSE loss\n1.0942436456680298\n90\nMSE loss\n1.1393386125564575\n91\nMSE loss\n1.1596001386642456\n92\nMSE loss\n0.9701560139656067\nsuccessful iteration\n93\nMSE loss\n1.1864886283874512\n94\nMSE loss\n0.943788468837738\nsuccessful iteration\n95\nMSE loss\n1.1077839136123657\n96\nMSE loss\n1.1722873449325562\n97\nMSE loss\n1.1074782609939575\nsuccessful iteration\n98\nMSE loss\n1.173482060432434\n99\nMSE loss\n1.097773551940918\nsuccessful iteration\n100\nMSE loss\n1.3038138151168823\n101\nMSE loss\n1.0825828313827515\nsuccessful iteration\n102\nMSE loss\n1.1240184307098389\n103\nMSE loss\n1.107921838760376\nsuccessful iteration\n104\nMSE loss\n1.2521613836288452\n105\nMSE loss\n1.2627928256988525\n106\nMSE loss\n1.3168892860412598\n107\nMSE loss\n1.1134288311004639\nsuccessful iteration\n108\nMSE loss\n1.0613383054733276\nsuccessful iteration\n109\nMSE loss\n1.2133185863494873\n110\nMSE loss\n1.1966614723205566\nsuccessful iteration\n111\nMSE loss\n1.1943005323410034\nsuccessful iteration\n112\nMSE loss\n0.9858878254890442\nsuccessful iteration\n113\nMSE loss\n1.1469988822937012\n114\nMSE loss\n1.1355730295181274\nsuccessful iteration\n115\nMSE loss\n1.2509441375732422\n116\nMSE loss\n1.1156400442123413\nsuccessful iteration\n117\nMSE loss\n1.2442781925201416\n118\nMSE loss\n1.0329722166061401\nsuccessful iteration\n119\nMSE loss\n0.9871751070022583\nsuccessful iteration\n120\nMSE loss\n1.0314558744430542\n121\nMSE loss\n1.1940295696258545\n122\nMSE loss\n1.0568246841430664\nsuccessful iteration\n123\nMSE loss\n0.872690737247467\nsuccessful iteration\n124\nMSE loss\n1.092221736907959\n125\nMSE loss\n0.8562747240066528\nsuccessful iteration\n126\nMSE loss\n0.8311471343040466\nsuccessful iteration\n127\nMSE loss\n0.8236032724380493\nsuccessful iteration\n128\nMSE loss\n1.0201746225357056\n129\nMSE loss\n0.790696918964386\nsuccessful iteration\n130\nMSE loss\n0.9944373369216919\n131\nMSE loss\n1.0430302619934082\n132\nMSE loss\n0.8033616542816162\nsuccessful iteration\n133\nMSE loss\n0.7440819144248962\nsuccessful iteration\n134\nMSE loss\n0.7117707133293152\nsuccessful iteration\n135\nMSE loss\n0.6468034982681274\nsuccessful iteration\n136\nMSE loss\n0.8784284591674805\n137\nMSE loss\n0.8129530549049377\nsuccessful iteration\n138\nMSE loss\n0.8856214284896851\n139\nMSE loss\n0.7623688578605652\nsuccessful iteration\n140\nMSE loss\n0.5667552947998047\nsuccessful iteration\n141\nMSE loss\n0.6604735851287842\n142\nMSE loss\n0.5988205075263977\nsuccessful iteration\n143\nMSE loss\n0.6571761965751648\n144\nMSE loss\n0.6390199065208435\nsuccessful iteration\n145\nMSE loss\n0.713194727897644\n146\nMSE loss\n0.6517559289932251\nsuccessful iteration\n147\nMSE loss\n0.6397010684013367\nsuccessful iteration\n148\nMSE loss\n0.4123488664627075\nsuccessful iteration\n149\nMSE loss\n0.4799264669418335\n150\nMSE loss\n0.4728482961654663\nsuccessful iteration\n151\nMSE loss\n0.4625896215438843\nsuccessful iteration\n152\nMSE loss\n0.5639962553977966\n153\nMSE loss\n0.3969406187534332\nsuccessful iteration\n154\nMSE loss\n0.4452172815799713\n155\nMSE loss\n0.49863603711128235\n156\nMSE loss\n0.33172935247421265\nsuccessful iteration\n157\nMSE loss\n0.3307066261768341\nsuccessful iteration\n158\nMSE loss\n0.41667941212654114\n159\nMSE loss\n0.34402692317962646\nsuccessful iteration\n160\nMSE loss\n0.3762017786502838\n161\nMSE loss\n0.34960871934890747\nsuccessful iteration\n162\nMSE loss\n0.20310188829898834\nsuccessful iteration\n163\nMSE loss\n0.20310065150260925\nsuccessful iteration\n164\nMSE loss\n0.2546460032463074\n165\nMSE loss\n0.11001791059970856\nsuccessful iteration\n166\nMSE loss\n0.2499585747718811\n167\nMSE loss\n0.08592256158590317\nsuccessful iteration\n168\nMSE loss\n0.16047275066375732\n169\nMSE loss\n0.06392215192317963\nsuccessful iteration\n170\nMSE loss\n0.07545618712902069\n171\nMSE loss\n0.1643795222043991\n172\nMSE loss\n0.04743973910808563\nsuccessful iteration\n173\nMSE loss\n0.1085413321852684\n174\nMSE loss\n0.07379316538572311\nsuccessful iteration\n175\nMSE loss\n0.03138476610183716\nsuccessful iteration\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-648443d669cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e0fb3dd3b144>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, f, J, alpha, closure)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# calculate the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdelta_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_loss = float('inf')\n",
    "alpha = 0.5\n",
    "for i, (data, target) in enumerate(torch_dataset):\n",
    "    print (i)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = net(data)\n",
    "    diff = out - target\n",
    "    loss = (diff ** 2).item()\n",
    "    print ('MSE loss')\n",
    "    print (loss)\n",
    "    if loss < prev_loss:\n",
    "        print ('successful iteration')\n",
    "        if alpha > 1e-5:\n",
    "            alpha /= 10\n",
    "    else:\n",
    "        if alpha < 1e5:\n",
    "            alpha *= 10\n",
    "    prev_loss = loss\n",
    "    J = jacobian(diff, net.parameters())\n",
    "    optimizer.step(diff, J, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "second_opt",
   "display_name": "second_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}