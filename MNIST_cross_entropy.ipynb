{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "class MNIST_small(Dataset):\n",
    "    \n",
    "    def __init__(self, train=True):\n",
    "        digits = datasets.load_digits()\n",
    "        data_num = digits['target'].shape[0]\n",
    "        if train == True:\n",
    "            self.data, self.target = digits['data'][:int(0.8*data_num)], digits['target'][:int(0.8*data_num)]\n",
    "        else:\n",
    "            self.data, self.target = digits['data'][int(0.8*data_num):], digits['target'][int(0.8*data_num):]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(self.data[idx]).float()\n",
    "        target = torch.from_numpy(np.asarray(self.target[idx]))\n",
    "        sample = {'data': data, 'target': target}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "trainset = MNIST_small(train=True)\n",
    "testset = MNIST_small(train=False)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda:2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss:2.7432808876037598\nloss:2.5503458976745605\nloss:2.3574624061584473\nloss:2.2161707878112793\nloss:2.180281639099121\nloss:2.1399340629577637\nloss:2.136575937271118\nloss:1.9382085800170898\nloss:1.9730643033981323\nloss:1.8597010374069214\nloss:1.8164050579071045\nloss:1.827252984046936\nloss:1.7980520725250244\nloss:1.6091443300247192\nloss:1.6104958057403564\nloss:1.5744025707244873\nloss:1.6058464050292969\nloss:1.5472010374069214\ntesting...\n0.49722222222222223\n"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 40)\n",
    "        self.fc2 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def AccuarcyCompute(pred,label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for idx, batch in enumerate (trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        print ('loss:{}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "import time\n",
    "\n",
    "class LM(Optimizer):\n",
    "    '''\n",
    "    Arguments:\n",
    "        lr: learning rate (step size) default:1\n",
    "        alpha: the hyperparameter in the regularization default:0.2\n",
    "    '''\n",
    "    def __init__(self, params, lr=1, alpha=0.2):\n",
    "        defaults = dict(\n",
    "            lr = lr,\n",
    "            alpha = alpha\n",
    "        )\n",
    "        super(LM, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError (\"LM doesn't support per-parameter options\") \n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        '''\n",
    "        performs a single step\n",
    "        in the closure: we approximate the Hessian for cross entropy loss\n",
    "\n",
    "        '''\n",
    "        assert len(self.param_groups) == 1\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        alpha = group['alpha']\n",
    "        params = group['params']\n",
    "\n",
    "\n",
    "        time_start=time.time()\n",
    "        prev_loss, g, H = closure(sample=True)\n",
    "        time_end=time.time()\n",
    "        print('sampling time',time_end-time_start,'s')\n",
    "        \n",
    "        H += torch.eye(H.shape[0]).to(device)*alpha\n",
    "\n",
    "        time_start = time.time()\n",
    "        delta_w = -1 * torch.matmul(torch.inverse(H), g).detach()\n",
    "        time_end = time.time()\n",
    "        print('inverting time',time_end-time_start,'s')\n",
    "\n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            with torch.no_grad():\n",
    "                p.add_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "            offset += numel\n",
    "\n",
    "        loss = closure(sample=False)\n",
    "\n",
    "        if loss < prev_loss:\n",
    "            print(loss.item())\n",
    "            print ('successful iteration')\n",
    "            if alpha > 1e-5:\n",
    "                group['alpha'] /= 10\n",
    "        else:\n",
    "            print ('failed iteration')\n",
    "            if alpha < 1e5:\n",
    "                group['alpha'] *= 10\n",
    "            # undo the step\n",
    "            offset = 0\n",
    "            for p in self._params:    \n",
    "                numel = p.numel()\n",
    "                with torch.no_grad():\n",
    "                    p.sub_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "                offset += numel\n",
    "\n",
    "\n",
    "def gather_flat_grad(params):\n",
    "    views=[]\n",
    "    for p in params:\n",
    "        if p.grad is None:\n",
    "            view = p.new(p.numel()).zero_()\n",
    "        else:\n",
    "            view = p.grad.view(-1)\n",
    "        views.append(view)\n",
    "    \n",
    "    return torch.cat(views, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sampling time 0.09043478965759277 s\ninverting time 0.8497962951660156 s\n2.6508350372314453\nsuccessful iteration\nsampling time 0.1419360637664795 s\ninverting time 0.7393507957458496 s\n1.93453049659729\nsuccessful iteration\nsampling time 0.1215667724609375 s\ninverting time 0.6907782554626465 s\n1.4909210205078125\nsuccessful iteration\nsampling time 0.13475823402404785 s\ninverting time 1.1203365325927734 s\nfailed iteration\nsampling time 0.10550212860107422 s\ninverting time 0.5628073215484619 s\nfailed iteration\nsampling time 0.12768220901489258 s\ninverting time 0.6146109104156494 s\n1.0897706747055054\nsuccessful iteration\nsampling time 0.14484047889709473 s\ninverting time 0.5620076656341553 s\nfailed iteration\nsampling time 0.11862564086914062 s\ninverting time 0.503312349319458 s\n0.9412541389465332\nsuccessful iteration\nsampling time 0.11890959739685059 s\ninverting time 0.7089035511016846 s\nfailed iteration\nsampling time 0.08351492881774902 s\ninverting time 1.1188771724700928 s\n0.6532264947891235\nsuccessful iteration\nsampling time 0.10140800476074219 s\ninverting time 0.7277650833129883 s\nfailed iteration\nsampling time 0.13525080680847168 s\ninverting time 0.571460485458374 s\n0.5206750631332397\nsuccessful iteration\nsampling time 0.15567231178283691 s\ninverting time 0.6984002590179443 s\nfailed iteration\nsampling time 0.13811588287353516 s\ninverting time 0.8678479194641113 s\n0.45347464084625244\nsuccessful iteration\nsampling time 0.1667795181274414 s\ninverting time 0.7707016468048096 s\nfailed iteration\nsampling time 0.11616778373718262 s\ninverting time 0.5972208976745605 s\n0.38794422149658203\nsuccessful iteration\nsampling time 0.11972379684448242 s\ninverting time 0.9635469913482666 s\nfailed iteration\nsampling time 0.1363821029663086 s\ninverting time 0.9639625549316406 s\nfailed iteration\ntesting...\n0.6694444444444444\n"
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = LM(model.parameters(),lr=1, alpha=1)\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for idx,data in enumerate(trainloader):    \n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        # evaluation H and g in a mini-batch\n",
    "        def closure(sample=True):\n",
    "            N = inputs.shape[0]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if sample:\n",
    "                for i in range(N):\n",
    "                    optimizer.zero_grad()\n",
    "                    grad = torch.autograd.grad(loss[i], model.parameters(), retain_graph=True, allow_unused=True)\n",
    "                    g_i = torch.cat([x.contiguous().view(-1, 1) for x in grad])\n",
    "                    H_i = torch.matmul(g_i, g_i.T)\n",
    "                    if i==0:\n",
    "                        H = H_i / N\n",
    "                        g = g_i / N\n",
    "                    else:\n",
    "                        H += H_i / N\n",
    "                        g += g_i / N\n",
    "                return torch.sum(loss)/N, g, H\n",
    "            else:\n",
    "                return torch.sum(loss)/N\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "second_opt",
   "display_name": "second_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}