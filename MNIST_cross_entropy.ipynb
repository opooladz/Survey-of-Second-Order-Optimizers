{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "class MNIST_small(Dataset):\n",
    "    \n",
    "    def __init__(self, train=True):\n",
    "        digits = datasets.load_digits()\n",
    "        data_num = digits['target'].shape[0]\n",
    "        if train == True:\n",
    "            self.data, self.target = digits['data'][:int(0.8*data_num)], digits['target'][:int(0.8*data_num)]\n",
    "        else:\n",
    "            self.data, self.target = digits['data'][int(0.8*data_num):], digits['target'][int(0.8*data_num):]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(self.data[idx]).float()\n",
    "        target = torch.from_numpy(np.asarray(self.target[idx]))\n",
    "        sample = {'data': data, 'target': target}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "trainset = MNIST_small(train=True)\n",
    "testset = MNIST_small(train=False)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda:2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss:2.68456768989563\nloss:2.558706045150757\nloss:2.525189161300659\nloss:2.270024538040161\nloss:2.2143473625183105\nloss:2.2888429164886475\nloss:2.18802809715271\nloss:2.1403424739837646\nloss:2.073456048965454\nloss:2.0570931434631348\nloss:1.883184790611267\nloss:2.0486807823181152\nloss:1.8677685260772705\nloss:2.0030908584594727\nloss:1.7067409753799438\nloss:1.7043460607528687\nloss:1.8648264408111572\nloss:1.6157644987106323\nloss:1.5672208070755005\nloss:1.3406749963760376\nloss:1.4936156272888184\nloss:1.4134997129440308\nloss:1.4630792140960693\nloss:1.2997300624847412\nloss:1.4504523277282715\nloss:1.3156503438949585\nloss:1.3700509071350098\nloss:1.2275644540786743\nloss:1.1649738550186157\nloss:1.2308419942855835\nloss:1.3489254713058472\nloss:1.1250503063201904\nloss:1.2172441482543945\nloss:1.1545486450195312\nloss:0.979830265045166\nloss:1.2308664321899414\nloss:1.0670419931411743\nloss:1.1510714292526245\nloss:1.1419411897659302\nloss:1.2602543830871582\nloss:1.0467212200164795\nloss:1.0526807308197021\nloss:0.974148690700531\nloss:1.1165342330932617\nloss:1.0374362468719482\nloss:1.0573711395263672\nloss:1.004192590713501\nloss:0.7175667881965637\nloss:1.1213403940200806\nloss:1.0861835479736328\nloss:1.0278364419937134\nloss:1.185192584991455\nloss:1.1652816534042358\nloss:1.0776655673980713\nloss:0.8943237066268921\nloss:0.8675366044044495\nloss:1.022408127784729\nloss:0.9795622825622559\nloss:0.7866591811180115\nloss:0.7206395864486694\nloss:1.0839884281158447\nloss:0.811488151550293\nloss:1.1580615043640137\nloss:1.0089025497436523\nloss:1.0500215291976929\nloss:0.8399618864059448\nloss:1.092279314994812\nloss:0.9373151659965515\nloss:0.8732532262802124\nloss:0.9247751235961914\nloss:0.7860634326934814\nloss:0.768220067024231\nloss:1.0071924924850464\nloss:0.8585279583930969\nloss:0.7650269865989685\nloss:0.7961450815200806\nloss:1.0445305109024048\nloss:0.784658670425415\nloss:0.7571873664855957\nloss:0.7427300214767456\nloss:0.8119310140609741\nloss:0.7845368385314941\nloss:0.828208327293396\nloss:0.7479733824729919\nloss:0.7917609810829163\nloss:0.8635333180427551\nloss:0.7997950315475464\nloss:0.7252333760261536\nloss:0.7273816466331482\nloss:0.9071046710014343\nloss:0.6432288289070129\nloss:0.8405597805976868\nloss:0.6685162782669067\nloss:0.7436773180961609\nloss:0.7844657301902771\nloss:0.5699648261070251\nloss:0.5933237671852112\nloss:0.8974353671073914\nloss:0.6915448904037476\nloss:0.8529794216156006\nloss:0.6124147772789001\nloss:0.8045215606689453\nloss:0.7593212723731995\nloss:0.8256593942642212\nloss:0.6880385279655457\nloss:0.7544454336166382\nloss:0.7800301909446716\nloss:0.6406156420707703\nloss:0.5955207943916321\nloss:0.8244463801383972\nloss:0.7667263150215149\nloss:0.7667478322982788\nloss:0.7025963664054871\nloss:0.7975382208824158\nloss:0.6057934761047363\nloss:0.6093127131462097\nloss:0.9286352396011353\nloss:0.6509387493133545\nloss:0.7625593543052673\nloss:0.7921566963195801\nloss:0.7393605709075928\nloss:0.7934812903404236\nloss:0.8522072434425354\nloss:0.6474073529243469\nloss:0.7683609127998352\nloss:0.5916262269020081\nloss:0.8695463538169861\nloss:0.6689775586128235\nloss:0.5817323923110962\nloss:0.7602351307868958\nloss:0.6811559200286865\nloss:0.49333104491233826\nloss:0.7180452942848206\nloss:0.5337185263633728\nloss:0.7113630771636963\nloss:0.710849940776825\nloss:0.6567585468292236\nloss:0.7418584227561951\nloss:0.8176863193511963\nloss:0.817897617816925\nloss:0.7087386846542358\nloss:0.7880979180335999\nloss:0.5378151535987854\nloss:1.258044719696045\nloss:0.6948385834693909\nloss:0.6911813616752625\nloss:0.6571976542472839\nloss:0.7294655442237854\nloss:0.7592480182647705\nloss:0.709801197052002\nloss:0.7919110655784607\nloss:0.8158429861068726\nloss:0.685425341129303\nloss:0.6140040159225464\nloss:0.6593224406242371\nloss:0.7463141679763794\nloss:0.6414638757705688\nloss:0.7884687781333923\nloss:0.720059335231781\nloss:0.7102790474891663\nloss:0.6109335422515869\nloss:0.7147031426429749\nloss:0.6898640394210815\nloss:0.5899530053138733\nloss:0.8034229278564453\nloss:0.7127981781959534\nloss:0.7663673758506775\nloss:0.7957530617713928\nloss:0.6698733568191528\nloss:0.7113094329833984\nloss:0.6534551978111267\nloss:0.7760211825370789\nloss:0.6560255885124207\nloss:0.7393171787261963\nloss:0.823245108127594\nloss:0.5942663550376892\nloss:0.7200804948806763\nloss:0.7511722445487976\nloss:0.6302134990692139\nloss:0.7472373247146606\nloss:0.7477641105651855\nloss:0.6697760224342346\nloss:0.6074784398078918\nloss:0.7790267467498779\nloss:0.678520679473877\nloss:0.552949070930481\nloss:0.7306772470474243\nloss:0.7194133996963501\nloss:0.7000061273574829\nloss:0.6585859060287476\nloss:0.7852311134338379\nloss:0.7494685649871826\nloss:0.6703444719314575\nloss:0.7060391902923584\nloss:0.7597399353981018\nloss:0.6998894810676575\nloss:0.6791560053825378\nloss:0.6251977682113647\nloss:0.7399042844772339\nloss:0.742467999458313\nloss:0.5631552934646606\nloss:0.59629887342453\nloss:0.8083659410476685\nloss:0.908285915851593\nloss:0.7795578241348267\nloss:0.5770733952522278\nloss:0.727450430393219\nloss:0.7024135589599609\nloss:0.7498674392700195\nloss:0.6394625902175903\nloss:0.7409855127334595\nloss:0.6743732690811157\nloss:0.8204451203346252\nloss:0.663520872592926\nloss:0.5763382315635681\nloss:0.44457173347473145\nloss:0.6009514331817627\nloss:0.8089430332183838\nloss:0.5771539807319641\nloss:0.8190850615501404\nloss:0.8472251296043396\nloss:0.6040672063827515\nloss:0.6005532145500183\nloss:0.6679400205612183\nloss:0.5250920653343201\nloss:0.738088071346283\nloss:0.8051216006278992\nloss:0.7611900568008423\nloss:0.807131826877594\nloss:0.5954410433769226\nloss:0.589779257774353\nloss:0.7500587701797485\nloss:0.5242000222206116\nloss:0.8097203969955444\nloss:0.6511321663856506\nloss:0.6929260492324829\nloss:0.8206110000610352\nloss:0.6577672958374023\nloss:0.6705380082130432\nloss:0.7193070650100708\ntesting...\n0.6694444444444444\n"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 40)\n",
    "        self.fc2 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def AccuarcyCompute(pred,label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for idx, batch in enumerate (trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        print ('loss:{}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class LM(Optimizer):\n",
    "    '''\n",
    "    Arguments:\n",
    "        lr: learning rate (step size) default:1\n",
    "        alpha: the hyperparameter in the regularization default:0.2\n",
    "    '''\n",
    "    def __init__(self, params, lr=1, alpha=0.2):\n",
    "        defaults = dict(\n",
    "            lr = lr,\n",
    "            alpha = alpha\n",
    "        )\n",
    "        super(LM, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError (\"LM doesn't support per-parameter options\") \n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        '''\n",
    "        performs a single step\n",
    "        in the closure: we approximate the Hessian for cross entropy loss\n",
    "\n",
    "        '''\n",
    "        assert len(self.param_groups) == 1\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        alpha = group['alpha']\n",
    "        params = group['params']\n",
    "\n",
    "        prev_loss, g, H = closure(sample=True)\n",
    "        H += torch.eye(H.shape[0]).to(device)*alpha\n",
    "        delta_w = -1 * torch.matmul(torch.inverse(H), g).detach()\n",
    "        \n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            with torch.no_grad():\n",
    "                p.add_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "            offset += numel\n",
    "\n",
    "        loss = closure(sample=False)\n",
    "\n",
    "        if loss < prev_loss:\n",
    "            print(loss.item())\n",
    "            print ('successful iteration')\n",
    "            if alpha > 1e-5:\n",
    "                group['alpha'] /= 10\n",
    "        else:\n",
    "            print ('failed iteration')\n",
    "            if alpha < 1e5:\n",
    "                group['alpha'] *= 10\n",
    "            # undo the step\n",
    "            offset = 0\n",
    "            for p in self._params:    \n",
    "                numel = p.numel()\n",
    "                with torch.no_grad():\n",
    "                    p.sub_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "                offset += numel\n",
    "\n",
    "\n",
    "def gather_flat_grad(params):\n",
    "    views=[]\n",
    "    for p in params:\n",
    "        if p.grad is None:\n",
    "            view = p.new(p.numel()).zero_()\n",
    "        else:\n",
    "            view = p.grad.view(-1)\n",
    "        views.append(view)\n",
    "    \n",
    "    return torch.cat(views, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(2.3458, device='cuda:2')\nsuccessful iteration\ntensor(1.7412, device='cuda:2')\nsuccessful iteration\ntensor(1.5835, device='cuda:2')\nsuccessful iteration\nfailed iteration\nfailed iteration\ntensor(1.3722, device='cuda:2')\nsuccessful iteration\nfailed iteration\nfailed iteration\ntensor(1.1818, device='cuda:2')\nsuccessful iteration\ntensor(1.1088, device='cuda:2')\nsuccessful iteration\nfailed iteration\nfailed iteration\ntensor(0.9991, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.9544, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.9357, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.8606, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.8081, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.7646, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.7246, device='cuda:2')\nsuccessful iteration\nfailed iteration\nfailed iteration\ntensor(0.7169, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.7158, device='cuda:2')\nsuccessful iteration\ntensor(0.7157, device='cuda:2')\nsuccessful iteration\nfailed iteration\nfailed iteration\ntensor(0.7132, device='cuda:2')\nsuccessful iteration\nfailed iteration\ntensor(0.7124, device='cuda:2')\nsuccessful iteration\ntesting...\n0.5777777777777777\n"
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = LM(model.parameters(),lr=1, alpha=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for idx,data in enumerate(trainloader):    \n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        # evaluation H and g in a mini-batch\n",
    "        def closure(sample=True):\n",
    "            N = inputs.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            if sample:\n",
    "                loss.backward()\n",
    "                g = gather_flat_grad(model.parameters())\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    inputs_i, labels_i = torch.unsqueeze(inputs[i], 0), torch.unsqueeze(labels[i], 0)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs_i = model(inputs_i)\n",
    "                    loss_i = criterion(outputs_i, labels_i)\n",
    "                    loss_i.backward()\n",
    "                    flat_grad = gather_flat_grad(model.parameters()).view(-1, 1)\n",
    "                    H_i = torch.matmul(flat_grad, flat_grad.T)\n",
    "                    if i==0:\n",
    "                        H = H_i / N\n",
    "                    else:\n",
    "                        H += H_i / N\n",
    "                return loss.data, g, H\n",
    "            else:\n",
    "                return loss.data\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "263"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "second_opt",
   "display_name": "second_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}