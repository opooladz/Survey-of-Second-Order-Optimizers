{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sample_grad import *\n",
    "from sklearn import datasets\n",
    "import time\n",
    "\n",
    "class MNIST_small(Dataset):\n",
    "    \n",
    "    def __init__(self, train=True):\n",
    "        digits = datasets.load_digits()\n",
    "        data_num = digits['target'].shape[0]\n",
    "        if train == True:\n",
    "            self.data, self.target = digits['data'][:int(0.8*data_num)], digits['target'][:int(0.8*data_num)]\n",
    "        else:\n",
    "            self.data, self.target = digits['data'][int(0.8*data_num):], digits['target'][int(0.8*data_num):]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(self.data[idx]).float()\n",
    "        target = torch.from_numpy(np.asarray(self.target[idx]))\n",
    "        sample = {'data': data, 'target': target}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "trainset = MNIST_small(train=True)\n",
    "testset = MNIST_small(train=False)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda:2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss:2.7769970893859863\none step time 0.48224949836730957 s\nloss:2.6728012561798096\none step time 0.0016546249389648438 s\nloss:2.4737870693206787\none step time 0.001741170883178711 s\nloss:2.256592273712158\none step time 0.0017704963684082031 s\nloss:2.1517393589019775\none step time 0.0018677711486816406 s\nloss:2.0385594367980957\none step time 0.0017876625061035156 s\nloss:2.00132155418396\none step time 0.0019638538360595703 s\nloss:1.955230712890625\none step time 0.0018661022186279297 s\nloss:1.857215404510498\none step time 0.0017366409301757812 s\nloss:1.8027675151824951\none step time 0.0017311573028564453 s\nloss:1.5695836544036865\none step time 0.0016179084777832031 s\nloss:1.5444817543029785\none step time 0.0023233890533447266 s\nloss:1.6557073593139648\none step time 0.0018928050994873047 s\nloss:1.4571723937988281\none step time 0.0019516944885253906 s\nloss:1.4737539291381836\none step time 0.0017774105072021484 s\nloss:1.4272304773330688\none step time 0.00174713134765625 s\nloss:1.5021209716796875\none step time 0.00213623046875 s\nloss:1.2893626689910889\none step time 0.002056598663330078 s\ntesting...\n0.48055555555555557\n"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 40)\n",
    "        self.fc2 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def AccuarcyCompute(pred,label):\n",
    "    pred = pred.cpu().data.numpy()\n",
    "    label = label.cpu().data.numpy()\n",
    "    test_np = (np.argmax(pred,1) == label)\n",
    "    test_np = np.float32(test_np)\n",
    "    return np.mean(test_np)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for idx, batch in enumerate (trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        time_start = time.time()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        print ('loss:{}'.format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_end = time.time()\n",
    "        print('one step time',time_end-time_start,'s')\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "import time\n",
    "\n",
    "class LM(Optimizer):\n",
    "    '''\n",
    "    Arguments:\n",
    "        lr: learning rate (step size) default:1\n",
    "        alpha: the hyperparameter in the regularization default:0.2\n",
    "    '''\n",
    "    def __init__(self, params, lr=1, alpha=0.2):\n",
    "        defaults = dict(\n",
    "            lr = lr,\n",
    "            alpha = alpha\n",
    "        )\n",
    "        super(LM, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) != 1:\n",
    "            raise ValueError (\"LM doesn't support per-parameter options\") \n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        '''\n",
    "        performs a single step\n",
    "        in the closure: we approximate the Hessian for cross entropy loss\n",
    "\n",
    "        '''\n",
    "        assert len(self.param_groups) == 1\n",
    "        group = self.param_groups[0]\n",
    "        lr = group['lr']\n",
    "        alpha = group['alpha']\n",
    "        params = group['params']\n",
    "\n",
    "        prev_loss, g, H = closure(sample=True)\n",
    "        \n",
    "        H += torch.eye(H.shape[0]).to(device)*alpha\n",
    "\n",
    "        time_start = time.time()\n",
    "        delta_w = -1 * torch.matmul(torch.inverse(H), g).detach()\n",
    "        time_end = time.time()\n",
    "        print('inverting time',time_end-time_start,'s')\n",
    "\n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            with torch.no_grad():\n",
    "                p.add_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "            offset += numel\n",
    "\n",
    "        loss = closure(sample=False)\n",
    "\n",
    "        if loss < prev_loss:\n",
    "            print(loss.item())\n",
    "            print ('successful iteration')\n",
    "            if alpha > 1e-5:\n",
    "                group['alpha'] /= 10\n",
    "        else:\n",
    "            print ('failed iteration')\n",
    "            if alpha < 1e5:\n",
    "                group['alpha'] *= 10\n",
    "            # undo the step\n",
    "            offset = 0\n",
    "            for p in self._params:    \n",
    "                numel = p.numel()\n",
    "                with torch.no_grad():\n",
    "                    p.sub_(delta_w[offset:offset + numel].view_as(p),alpha=lr)\n",
    "                offset += numel\n",
    "\n",
    "\n",
    "def gather_flat_grads(params):\n",
    "    '''\n",
    "    return a matrix with size (batch_size, param_num)\n",
    "    the flattened gradient of each sample in the batch \n",
    "\n",
    "    '''\n",
    "    views=[]\n",
    "    for p in params:\n",
    "        if p.grads is None:\n",
    "            view = torch.zeros_like(p.grads).view(p.grads.shape[0], -1)\n",
    "        else:\n",
    "            view = p.grads.view(p.grads.shape[0], -1)\n",
    "        views.append(view)\n",
    "    \n",
    "    return torch.cat(views, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sampling time 0.0027124881744384766 s\ninverting time 0.8638589382171631 s\n2.081667184829712\nsuccessful iteration\nsampling time 0.011427164077758789 s\ninverting time 1.0071516036987305 s\n1.667224407196045\nsuccessful iteration\nsampling time 0.0035326480865478516 s\ninverting time 0.5303542613983154 s\n1.592307209968567\nsuccessful iteration\nsampling time 0.005349159240722656 s\ninverting time 0.6843528747558594 s\nfailed iteration\nsampling time 0.003390789031982422 s\ninverting time 1.5429844856262207 s\n1.4501111507415771\nsuccessful iteration\nsampling time 0.006289958953857422 s\ninverting time 1.274895191192627 s\n1.3662605285644531\nsuccessful iteration\nsampling time 0.0091705322265625 s\ninverting time 1.2970750331878662 s\nfailed iteration\nsampling time 0.010812520980834961 s\ninverting time 0.8452491760253906 s\nfailed iteration\nsampling time 0.0077593326568603516 s\ninverting time 0.9346725940704346 s\n1.248013973236084\nsuccessful iteration\nsampling time 0.00551295280456543 s\ninverting time 1.0838475227355957 s\nfailed iteration\nsampling time 0.008353233337402344 s\ninverting time 1.1800053119659424 s\n1.12061607837677\nsuccessful iteration\nsampling time 0.00863194465637207 s\ninverting time 0.9907755851745605 s\n1.0872172117233276\nsuccessful iteration\nsampling time 0.017871856689453125 s\ninverting time 0.5934114456176758 s\nfailed iteration\nsampling time 0.012086153030395508 s\ninverting time 0.920323371887207 s\nfailed iteration\nsampling time 0.009564638137817383 s\ninverting time 0.7109849452972412 s\n0.9957160949707031\nsuccessful iteration\nsampling time 0.016928434371948242 s\ninverting time 0.8347737789154053 s\nfailed iteration\nsampling time 0.014792203903198242 s\ninverting time 0.7611947059631348 s\nfailed iteration\nsampling time 0.00969696044921875 s\ninverting time 0.8164291381835938 s\n0.9550361633300781\nsuccessful iteration\ntesting...\n0.5805555555555556\n"
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = LM(model.parameters(),lr=1, alpha=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for idx,data in enumerate(trainloader):    \n",
    "        inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "        # evaluation H and g in a mini-batch\n",
    "        def closure(sample=True):\n",
    "            N = inputs.shape[0]\n",
    "            if sample:\n",
    "                time_start = time.time()\n",
    "                optimizer.zero_grad()\n",
    "                with save_sample_grads(model):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    g_all = gather_flat_grads(model.parameters())\n",
    "                    g = g_all.sum(0)\n",
    "                    H = N*torch.einsum('ijk, ikl -> ijl', [torch.unsqueeze(g_all, 2), torch.unsqueeze(g_all, 1)]).sum(0)\n",
    "                    time_end = time.time()\n",
    "                    print('sampling time',time_end-time_start,'s')\n",
    "                    return torch.sum(loss), g, H\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                return torch.sum(loss)\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "\n",
    "print ('testing...')\n",
    "total, correct = 0, 0\n",
    "for idx, batch in enumerate(testloader):    \n",
    "    inputs, labels = batch['data'].to(device), batch['target'].to(device)\n",
    "    outputs = model(inputs)\n",
    "    correct += torch.sum(torch.argmax(outputs,1) == labels).item()\n",
    "    total += inputs.shape[0]\n",
    "print (correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "second_opt",
   "display_name": "second_opt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}